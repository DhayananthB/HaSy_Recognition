{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "gpuType": "T4",
      "authorship_tag": "ABX9TyMlrGROZhuzBFl9OgToy0OR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/DhayananthB/HaSy_Recognition/blob/main/Hasy.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "from torch import nn\n",
        "from torch.utils.data import DataLoader\n",
        "from torchvision.transforms import ToTensor\n",
        "import matplotlib.pyplot as plt\n",
        "from torchvision import transforms\n",
        "import torch.optim as optim"
      ],
      "metadata": {
        "id": "Df4je8wTKQW5"
      },
      "execution_count": 16,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 17,
      "metadata": {
        "id": "WbFLL8vsFoTJ"
      },
      "outputs": [],
      "source": [
        "# !pip install deeplake\n",
        "import deeplake\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Dataset Description\n",
        "- HASY (Handwritten Symbol Dataset)\n",
        "- 168,233 instances of 369 classes\n",
        "- 3X32x32  images"
      ],
      "metadata": {
        "id": "q4t1A1vD66n1"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Reading data"
      ],
      "metadata": {
        "id": "vUKHpvMo78yM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "ds = deeplake.load(\"hub://activeloop/hasy-train\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ao5D1zsiFrgI",
        "outputId": "3714b96a-5d9a-41ff-a998-78ba5f1064c7"
      },
      "execution_count": 18,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "|"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/hasy-train\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\\"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://activeloop/hasy-train loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": []
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "train sample size"
      ],
      "metadata": {
        "id": "6CndsBmX4lsj"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(ds)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "2usN-PcV0qdy",
        "outputId": "98e55fd6-f437-4525-c0f8-c9e7975474a4"
      },
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "151241"
            ]
          },
          "metadata": {},
          "execution_count": 19
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# ds.visualize()"
      ],
      "metadata": {
        "id": "IA8HJW5SMvHB"
      },
      "execution_count": 20,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dt = deeplake.load(\"hub://activeloop/hasy-test\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "-AByHwtmF_0T",
        "outputId": "9de0992f-209e-4472-8dd5-e0a523391698"
      },
      "execution_count": 21,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r\r"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Opening dataset in read-only mode as you don't have write permissions.\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "-"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "This dataset can be visualized in Jupyter Notebook by ds.visualize() or at https://app.activeloop.ai/activeloop/hasy-test\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "hub://activeloop/hasy-test loaded successfully.\n",
            "\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "\r \r\r\r"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "test sample size"
      ],
      "metadata": {
        "id": "klg0jZ6D4qeM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "len(dt)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vWPGGQ0G0DG4",
        "outputId": "ee21afd6-6fa2-4172-d6d9-f94a7370b97c"
      },
      "execution_count": 22,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "16992"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "### transformation"
      ],
      "metadata": {
        "id": "XZIvT99b8JT8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "transform_dict = {\n",
        "    'images': transforms.Compose([\n",
        "        transforms.ToTensor(),\n",
        "        transforms.Normalize((0.5,), (0.5,))  # Normalize with mean=0.5, std=0.5\n",
        "    ])\n",
        "}\n",
        "\n",
        "def custom_transform(sample):\n",
        "    for key, transform in transform_dict.items():\n",
        "        if key in sample:\n",
        "            sample[key] = transform(sample[key])\n",
        "    return sample"
      ],
      "metadata": {
        "id": "zQaUDMAvNqxO"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Data Loading"
      ],
      "metadata": {
        "id": "HMwT2qca6IrA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "train_dataloader = ds.pytorch(num_workers=0, batch_size=256, shuffle=True,decode_method={\n",
        "        'images': 'pil'\n",
        "    },transform=custom_transform)"
      ],
      "metadata": {
        "id": "DlIdLXAHGkUi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "test_dataloader = dt.pytorch(num_workers=0, batch_size=256, shuffle=True,decode_method={\n",
        "        'images': 'pil'\n",
        "    },transform=custom_transform)"
      ],
      "metadata": {
        "id": "eBq6cq2QIyxz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "dict(next(iter(test_dataloader))).keys()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "w-aRU9BC6AcA",
        "outputId": "9523cb6b-878d-46a8-8a9e-39f2da71dd01"
      },
      "execution_count": 26,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "dict_keys(['images', 'latex', 'index'])"
            ]
          },
          "metadata": {},
          "execution_count": 26
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Neural Network"
      ],
      "metadata": {
        "id": "dFt-5wMvNGY8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class HASYNet(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(HASYNet, self).__init__()\n",
        "        # Convolutional Layers\n",
        "        self.conv1 = nn.Conv2d(3, 32, kernel_size=3, padding=1)  # (3, 32, 32) -> (32, 32, 32)\n",
        "        self.conv2 = nn.Conv2d(32, 64, kernel_size=3, padding=1)  # (32, 32, 32) -> (64, 32, 32)\n",
        "        self.conv3 = nn.Conv2d(64, 128, kernel_size=3, padding=1)  # (64, 32, 32) -> (128, 32, 32)\n",
        "\n",
        "        # Pooling Layer\n",
        "        self.pool = nn.MaxPool2d(2, 2)  # Reduces size by half each time\n",
        "\n",
        "        # Calculate the size of the input to the fully connected layer\n",
        "        # After 3 pooling layers (assuming input size is 32x32):\n",
        "        # Size will be reduced to 32 / 2^3 = 4 (height and width)\n",
        "        self.fc1_input_size = 128 * 4 * 4  # (128 channels) * (4 height) * (4 width)\n",
        "\n",
        "        # Fully Connected Layers\n",
        "        self.fc1 = nn.Linear(self.fc1_input_size, 512)\n",
        "        self.fc2 = nn.Linear(512, 369)  # 369 classes for output\n",
        "\n",
        "        # Activation\n",
        "        self.relu = nn.ReLU()\n",
        "\n",
        "        # Dropout for regularization\n",
        "        self.dropout = nn.Dropout(0.5)\n",
        "\n",
        "    def forward(self, x):\n",
        "        # Forward pass through convolutional layers with ReLU and MaxPool\n",
        "        x = self.pool(self.relu(self.conv1(x)))  # (batch_size, 3, 32, 32) -> (batch_size, 32, 16, 16)\n",
        "        x = self.pool(self.relu(self.conv2(x)))  # (batch_size, 32, 16, 16) -> (batch_size, 64, 8, 8)\n",
        "        x = self.pool(self.relu(self.conv3(x)))  # (batch_size, 64, 8, 8) -> (batch_size, 128, 4, 4)\n",
        "\n",
        "        # Flatten the tensor\n",
        "        x = x.view(-1, self.fc1_input_size)  # (batch_size, 128*4*4)\n",
        "\n",
        "        # Fully Connected layers with ReLU and Dropout\n",
        "        x = self.relu(self.fc1(x))  # (batch_size, 512)\n",
        "        x = self.dropout(x)  # Apply Dropout\n",
        "        x = self.fc2(x)  # (batch_size, 369) -> Output layer\n",
        "\n",
        "        return x"
      ],
      "metadata": {
        "id": "b4y0cmrf6Ds3"
      },
      "execution_count": 27,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "device = 'cpu'\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ],
      "metadata": {
        "id": "fkcgVlCk-VVi"
      },
      "execution_count": 28,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HASYNet().to(device)\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)"
      ],
      "metadata": {
        "id": "pB-RMM9a8wsq"
      },
      "execution_count": 29,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "model = HASYNet()  # Assuming HASYNet is defined elsewhere\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "# Number of epochs\n",
        "num_epochs = 3\n",
        "\n",
        "# Function to evaluate loss and accuracy\n",
        "def evaluate_loss_and_accuracy(loader, model, criterion):\n",
        "    total_loss = 0.0\n",
        "    correct = 0\n",
        "    total = 0\n",
        "    model.eval()  # Set model to evaluation mode\n",
        "    with torch.no_grad():\n",
        "        for data in loader:\n",
        "            inputs = data['images']\n",
        "            labels = data['latex']\n",
        "\n",
        "            inputs.to(device)\n",
        "\n",
        "            # Ensure labels are in the correct format\n",
        "            labels = labels.squeeze()  # Convert from shape [128, 1] to [128]\n",
        "\n",
        "            labels.to(device)\n",
        "\n",
        "            outputs = model(inputs)\n",
        "            loss = criterion(outputs, labels)\n",
        "            total_loss += loss.item()\n",
        "\n",
        "            _, predicted = torch.max(outputs.data, 1)\n",
        "            total += labels.size(0)\n",
        "            correct += (predicted == labels).sum().item()\n",
        "\n",
        "    average_loss = total_loss / len(loader)\n",
        "    accuracy = 100 * correct / total\n",
        "    return average_loss, accuracy\n",
        "\n",
        "# Training and validation loop\n",
        "for epoch in range(num_epochs):\n",
        "    model.train()  # Set model to training mode\n",
        "    running_loss = 0.0\n",
        "\n",
        "    for i, data in enumerate(train_dataloader, 0):\n",
        "        inputs = data['images']\n",
        "        labels = data['latex']\n",
        "\n",
        "        inputs.to(device)\n",
        "\n",
        "        labels = labels.squeeze()\n",
        "         # Convert from shape [128, 1] to [128]\n",
        "\n",
        "        labels.to(device)\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "        outputs = model(inputs)\n",
        "        loss = criterion(outputs, labels)\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "        running_loss += loss.item()\n",
        "        if i % 100 == 99:  # Print every 100 mini-batches\n",
        "            print(f\"[Epoch {epoch + 1}, Batch {i + 1}] Training loss: {running_loss / 100:.3f}\")\n",
        "            running_loss = 0.0\n",
        "\n",
        "    # Calculate training loss and accuracy\n",
        "    train_loss, train_accuracy = evaluate_loss_and_accuracy(train_dataloader, model, criterion)\n",
        "    print(f'Epoch {epoch + 1} Training Loss: {train_loss:.4f}, Accuracy: {train_accuracy:.2f}%')\n",
        "\n",
        "    # Calculate validation loss and accuracy\n",
        "    val_loss, val_accuracy = evaluate_loss_and_accuracy(test_dataloader, model, criterion)\n",
        "    print(f'Epoch {epoch + 1} Validation Loss: {val_loss:.4f}, Accuracy: {val_accuracy:.2f}%')\n",
        "\n",
        "print('Finished Training')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "566wiXQNELIy",
        "outputId": "da5f5339-acf6-42da-d5c9-e152eb766a67"
      },
      "execution_count": 32,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "[Epoch 1, Batch 100] Training loss: 3.909\n",
            "[Epoch 1, Batch 200] Training loss: 1.813\n",
            "[Epoch 1, Batch 300] Training loss: 1.328\n",
            "[Epoch 1, Batch 400] Training loss: 1.135\n",
            "[Epoch 1, Batch 500] Training loss: 1.051\n",
            "Epoch 1 Training Loss: 0.6842, Accuracy: 79.88%\n",
            "Epoch 1 Validation Loss: 0.6649, Accuracy: 79.31%\n",
            "[Epoch 2, Batch 100] Training loss: 0.896\n",
            "[Epoch 2, Batch 200] Training loss: 0.886\n",
            "[Epoch 2, Batch 300] Training loss: 0.825\n",
            "[Epoch 2, Batch 400] Training loss: 0.851\n",
            "[Epoch 2, Batch 500] Training loss: 0.798\n",
            "Epoch 2 Training Loss: 0.5569, Accuracy: 82.82%\n",
            "Epoch 2 Validation Loss: 0.5710, Accuracy: 81.47%\n",
            "[Epoch 3, Batch 100] Training loss: 0.743\n",
            "[Epoch 3, Batch 200] Training loss: 0.733\n",
            "[Epoch 3, Batch 300] Training loss: 0.716\n",
            "[Epoch 3, Batch 400] Training loss: 0.727\n",
            "[Epoch 3, Batch 500] Training loss: 0.716\n",
            "Epoch 3 Training Loss: 0.4940, Accuracy: 84.17%\n",
            "Epoch 3 Validation Loss: 0.5254, Accuracy: 82.32%\n",
            "Finished Training\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "torch.save(model.state_dict(), \"hasy.pth\")"
      ],
      "metadata": {
        "id": "hyO6DJx6Qowu"
      },
      "execution_count": 34,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# for epoch in range(1):\n",
        "#     running_loss = 0.0\n",
        "#     for i, data in enumerate(train_dataloader, 0):\n",
        "#         # Extracting the inputs and labels from the dictionary\n",
        "#         inputs = data['images']\n",
        "#         labels = data['latex']\n",
        "#         labels = labels.squeeze()\n",
        "#         optimizer.zero_grad()\n",
        "\n",
        "#         outputs = model(inputs)\n",
        "\n",
        "#         # print(outputs.shape)\n",
        "#         # print(labels.shape)\n",
        "\n",
        "#         loss = criterion(outputs, labels)\n",
        "#         loss.backward()\n",
        "#         optimizer.step()\n",
        "\n",
        "#         running_loss += loss.item()\n",
        "#         if i % 100 == 99:  # Print every 100 mini-batches\n",
        "#             print(f\"[{epoch + 1}, {i + 1}] loss: {running_loss / 100:.3f}\")\n",
        "#             running_loss = 0.0\n",
        "\n",
        "# print('Finished Training')\n"
      ],
      "metadata": {
        "id": "47I2DGks-etz"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "C7NRcqsq_0nI"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}